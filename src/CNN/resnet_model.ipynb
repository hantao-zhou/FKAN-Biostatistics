{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg4vT1BWLTD7",
        "outputId": "072c6caa-8b1f-4d99-c544-14ecdf964cbe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT4iG--NOpTw",
        "outputId": "9da41fdb-deca-42ac-9a54-360016c7fc1f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11680mFSNiEV",
        "outputId": "6650f38a-c94f-4c88-9d51-12d855984734"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:00<00:00, 566.32it/s]\n",
            "100%|██████████| 390/390 [00:00<00:00, 721.47it/s]\n",
            "100%|██████████| 3875/3875 [00:05<00:00, 684.72it/s]\n",
            "100%|██████████| 3875/3875 [00:06<00:00, 632.63it/s]\n",
            "100%|██████████| 8/8 [00:00<00:00, 624.75it/s]\n",
            "100%|██████████| 8/8 [00:00<00:00, 714.02it/s]\n"
          ]
        }
      ],
      "source": [
        "labels = ['NORMAL', 'PNEUMONIA']\n",
        "img_size = 224\n",
        "\n",
        "def get_training_data(data_dir):\n",
        "    data = []\n",
        "\n",
        "    for label in labels:\n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                # Load and resize the image\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size))  # Resize the image\n",
        "\n",
        "                # Add the image and label as a pair\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img}: {e}\")\n",
        "\n",
        "    # Convert the list to a NumPy array\n",
        "    data = np.array(data, dtype=object)  # Use dtype=object to allow image-label pairing\n",
        "    return data\n",
        "\n",
        "# Load the data\n",
        "test_data = get_training_data('/Users/dangerdani/Documents/FKAN-Biostatistics/normData/test')\n",
        "train_data = get_training_data('/Users/dangerdani/Documents/FKAN-Biostatistics/normData/train')\n",
        "val_data = get_training_data('/Users/dangerdani/Documents/FKAN-Biostatistics/normData/val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrf_UaBJOHyK",
        "outputId": "4252c3db-2e24-435b-fc40-f89cdc568c90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7750/7750 [00:02<00:00, 2697.05it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 2964.04it/s]\n",
            "100%|██████████| 624/624 [00:00<00:00, 3391.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of normalized and shuffled train images: (7750, 224, 224)\n",
            "Shape of normalized and shuffled validation images: (16, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "# Function to normalize the images\n",
        "def normalize_images(data):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for img, label in tqdm(data):\n",
        "        # Normalization: each pixel is divided by 255\n",
        "        normalized_img = img / 255.0\n",
        "        images.append(normalized_img)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the images and labels into separate arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# Normalize the images in the training dataset\n",
        "train_images, train_labels = normalize_images(train_data)\n",
        "val_images, val_labels = normalize_images(val_data)\n",
        "test_images, test_labels = normalize_images(test_data)\n",
        "\n",
        "\n",
        "# Check the shape and an example of the normalized and shuffled data\n",
        "print(f\"Shape of normalized and shuffled train images: {test_images.shape}\")\n",
        "print(f\"Shape of normalized and shuffled train images: {train_images.shape}\")\n",
        "print(f\"Shape of normalized and shuffled validation images: {val_images.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhWclu08OZ6l",
        "outputId": "d5073850-3e5b-4686-fe54-830306e58ab2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dangerdani/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/dangerdani/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2, softmax=True):\n",
        "      super(ResNet, self).__init__()\n",
        "      self.resnet = torchvision.models.resnet50(pretrained=True)\n",
        "      num_ftrs = self.resnet.fc.out_features\n",
        "      self.fc = nn.Linear(num_ftrs, num_classes)\n",
        "      self.bn = nn.BatchNorm1d(num_ftrs)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.softmax = torch.nn.Softmax(dim=1) if softmax else None\n",
        "      self.change_conv1()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.resnet(x)\n",
        "      x = self.bn(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc(x)\n",
        "      if self.softmax:\n",
        "        x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "    def change_conv1(self):\n",
        "      original_conv1 = self.resnet.conv1\n",
        "\n",
        "      #Create a new convolutional layer with 1 input channel instead of 3\n",
        "      new_conv1 = nn.Conv2d(\n",
        "        in_channels=1,  # Grayscale has 1 channel\n",
        "        out_channels=original_conv1.out_channels,\n",
        "        kernel_size=original_conv1.kernel_size,\n",
        "        stride=original_conv1.stride,\n",
        "        padding=original_conv1.padding,\n",
        "        bias=original_conv1.bias is not None\n",
        ")\n",
        "\n",
        "      # Initialize the new conv layer's weights by averaging the RGB weights\n",
        "      with torch.no_grad():\n",
        "        new_conv1.weight = nn.Parameter(original_conv1.weight.mean(dim=1, keepdim=True))\n",
        "\n",
        "        #Replace the original conv1 with the new one\n",
        "        self.resnet.conv1 = new_conv1\n",
        "\n",
        "\n",
        "model = ResNet(num_classes=2, softmax=True)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDehAi3WS1xR",
        "outputId": "aae09fa0-0ab0-4ec3-c85c-df303a05980d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 1, 224, 224]) torch.Size([7750, 1, 224, 224])\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert the images and labels to PyTorch tensors\n",
        "\n",
        "# Apply the transformation to training and validation images\n",
        "train_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in train_images]).unsqueeze(1)\n",
        "val_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in val_images]).unsqueeze(1)\n",
        "\n",
        "# Now permute them\n",
        "train_images_tensor = train_images_tensor.permute(0, 1, 2, 3)  # (N, 1, 244, 244)\n",
        "val_images_tensor = val_images_tensor.permute(0, 1, 2, 3)      # (N, 1, 244, 244)\n",
        "print(val_images_tensor.shape, train_images_tensor.shape)\n",
        "\n",
        "# The tensors are now in the shape (N, 1, 244, 244), where N is the number of images\n",
        "\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "# Create the dataset and DataLoader\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "\n",
        "# Define the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "print('Done!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wCqaba_Xu59"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "oGlRJ8QtTziF",
        "outputId": "f22a306c-2bb0-4668-a84f-53b8a3ba7855"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - Loss: 0.35298936643551304: 100%|██████████| 485/485 [28:26<00:00,  3.52s/it]\n",
            "Epoch 2/100 - Loss: 0.328379967873045:  57%|█████▋    | 278/485 [15:54<11:50,  3.43s/it]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class or binary classification\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)  # AdamW with L2 regularization\n",
        "\n",
        "# Now the data is ready for training and validation\n",
        "\n",
        "# Function to calculate relevant metrics\n",
        "\n",
        "# Training function with Early Stopping\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
        "    patience_counter = 0\n",
        "    best_validation_score = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        p_bar = tqdm(train_loader)\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(p_bar):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            p_bar.set_description(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss / (i + 1)}\")\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            model.eval()\n",
        "            p_bar = tqdm(val_loader)\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "            with torch.no_grad():\n",
        "                for i, (images, labels) in enumerate(p_bar):\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    p_bar.set_description(f'Epoch {epoch+1}/{num_epochs} - Validation Batch: {i}')\n",
        "\n",
        "            class_report = classification_report(all_labels, all_preds, target_names=['Pneumonia', 'Normal'], output_dict=True)\n",
        "            validation_accuracy = class_report['accuracy']\n",
        "            validation_f1_score = class_report['weighted avg']['f1-score']\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Validation Accuracy: {validation_accuracy} - Validation F1 Score: {validation_f1_score:.4f}\")\n",
        "            if validation_f1_score > best_validation_score:\n",
        "                best_validation_score = validation_f1_score\n",
        "                patience_counter = 0\n",
        "                #torch.save(model.state_dict(), os.path.join('/content/drive/MyDrive/model_results', 'best_model_resnet.pth'))\n",
        "\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "# Start training\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ-Sw9n_Xoxx"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW6M8ESPYt7f",
        "outputId": "40b038d1-7ec6-4103-cfc7-c9c50afbcef5"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load('/content/drive/MyDrive/model_results/best_model_resnet.pth')\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAvunQT-Xtwx",
        "outputId": "81c71b55-645c-4d78-eb34-ed697dad9e35"
      },
      "outputs": [],
      "source": [
        "test_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in test_images]).unsqueeze(1)  # Applying the same transformation as for train/val\n",
        "test_images_tensor = test_images_tensor.permute(0, 1, 2, 3)\n",
        "print(test_images_tensor.shape)\n",
        "\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)  # or torch.float if binary classification\n",
        "\n",
        "# Create the dataset and DataLoader for the test set\n",
        "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for images, labels in test_loader:\n",
        "  images, labels = images.to(device), labels\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  all_predictions.extend(preds.cpu().numpy())\n",
        "  all_labels.extend(labels.numpy())\n",
        "\n",
        "class_report = classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA'])\n",
        "print(class_report)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FLEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
