{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg4vT1BWLTD7",
        "outputId": "1d38c2c9-53a9-4b15-f0bb-066896ab70f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT4iG--NOpTw",
        "outputId": "60cd962b-5123-426f-be02-7e87cc0d6931"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dangerdani/anaconda3/envs/FLEnv/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "#from google.colab import drive\n",
        "import albumentations as A\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11680mFSNiEV",
        "outputId": "de630698-5cd8-41f6-9bc7-012326fc2045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3107/3107 [00:15<00:00, 201.42it/s]\n",
            "100%|██████████| 3107/3107 [00:36<00:00, 85.05it/s] \n",
            "100%|██████████| 390/390 [00:01<00:00, 233.32it/s]\n",
            "100%|██████████| 234/234 [00:02<00:00, 96.95it/s] \n",
            "100%|██████████| 776/776 [00:03<00:00, 204.60it/s]\n",
            "100%|██████████| 270/270 [00:03<00:00, 82.31it/s]\n"
          ]
        }
      ],
      "source": [
        "labels = ['PNEUMONIA', 'NORMAL']\n",
        "img_size = 224\n",
        "\n",
        "def get_training_data(data_dir):\n",
        "    data = []\n",
        "\n",
        "    for label in labels:\n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                # Load and resize the image\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size))  # Resize the image\n",
        "\n",
        "                # Add the image and label as a pair\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img}: {e}\")\n",
        "\n",
        "    # Convert the list to a NumPy array\n",
        "    data = np.array(data, dtype=object)  # Use dtype=object to allow image-label pairing\n",
        "    return data\n",
        "\n",
        "# Load the data\n",
        "train_data = get_training_data('/Users/dangerdani/Documents/FKAN-Biostatistics/data/chest_xray/train')\n",
        "test_data = get_training_data('/Users/dangerdani/Documents/FKAN-Biostatistics/data/chest_xray/test')\n",
        "val_data = get_training_data('/Users/dangerdani/Documents/FKAN-Biostatistics/data/chest_xray/val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrf_UaBJOHyK",
        "outputId": "d800b490-6854-4753-ae82-b6625eed088b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6214/6214 [00:02<00:00, 2415.93it/s]\n",
            "100%|██████████| 1046/1046 [00:00<00:00, 1866.03it/s]\n",
            "100%|██████████| 624/624 [00:00<00:00, 1803.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of normalized and shuffled train images: (6214, 224, 224)\n",
            "Shape of normalized and shuffled validation images: (1046, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "# Function to normalize the images\n",
        "def normalize_images(data):\n",
        "    images = []\n",
        "    labels = []\n",
        "    normalizer = A.Normalize(mean=0.488, std=0.234, max_pixel_value=1)\n",
        "\n",
        "    for img, label in tqdm(data):\n",
        "        # Normalization: each pixel is divided by 255\n",
        "        normalized_img = img / 255.0\n",
        "        #normalized_img = normalizer(image=normalized_img)['image']\n",
        "        images.append(normalized_img)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the images and labels into separate arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# Normalize the images in the training dataset\n",
        "train_images, train_labels = normalize_images(train_data)\n",
        "val_images, val_labels = normalize_images(val_data)\n",
        "test_images, test_labels = normalize_images(test_data)\n",
        "\n",
        "\n",
        "# Check the shape and an example of the normalized and shuffled data\n",
        "print(f\"Shape of normalized and shuffled train images: {train_images.shape}\")\n",
        "print(f\"Shape of normalized and shuffled validation images: {val_images.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut-dAV3Ixrg5",
        "outputId": "9fa07f5a-7deb-44af-ab57-b5f8c0b70a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.187349\n",
            "-2.0854788\n"
          ]
        }
      ],
      "source": [
        "print(train_images[0].max())\n",
        "print(train_images[0].min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhWclu08OZ6l",
        "outputId": "ae97f186-7e21-470c-c5bc-f5521ae1829d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dangerdani/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/dangerdani/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/dangerdani/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:08<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2, softmax=True):\n",
        "      super(ResNet, self).__init__()\n",
        "      self.resnet = torchvision.models.resnet50(pretrained=True)\n",
        "      num_ftrs = self.resnet.fc.out_features  [B, N, H, W] -> [B, N * H * W] \n",
        "      self.fc = nn.Linear(num_ftrs, num_classes)\n",
        "      self.bn = nn.BatchNorm1d(num_ftrs)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.softmax = torch.nn.Softmax(dim=1) if softmax else None\n",
        "      self.change_conv1()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.resnet(x)\n",
        "      x = self.bn(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc(x)\n",
        "      if self.softmax:\n",
        "        x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "    def change_conv1(self):\n",
        "      original_conv1 = self.resnet.conv1\n",
        "\n",
        "      #Create a new convolutional layer with 1 input channel instead of 3\n",
        "      new_conv1 = nn.Conv2d(\n",
        "        in_channels=1,  # Grayscale has 1 channel\n",
        "        out_channels=original_conv1.out_channels,\n",
        "        kernel_size=original_conv1.kernel_size,\n",
        "        stride=original_conv1.stride,\n",
        "        padding=original_conv1.padding,\n",
        "        bias=original_conv1.bias is not None\n",
        ")\n",
        "\n",
        "      # Initialize the new conv layer's weights by averaging the RGB weights\n",
        "      with torch.no_grad():\n",
        "        new_conv1.weight = nn.Parameter(original_conv1.weight.mean(dim=1, keepdim=True))\n",
        "\n",
        "        #Replace the original conv1 with the new one\n",
        "        self.resnet.conv1 = new_conv1\n",
        "\n",
        "class KANLinear_v1(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=5, spline_order=3,\n",
        "                 scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
        "                 enable_standalone_scale_spline=True, base_activation=nn.SiLU,\n",
        "                 grid_eps=0.02, grid_range=[-1, 1]):\n",
        "        super(KANLinear_v1, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = ((torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                 + grid_range[0]).expand(in_features, -1).contiguous())\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
        "        )\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = ((torch.rand(self.grid_size + 1, self.in_features, self.out_features) - 0.5)\n",
        "                     * self.scale_noise / self.grid_size)\n",
        "            self.spline_weight.data.copy_(\n",
        "                self.scale_spline * self.curve2coeff(self.grid.T[self.spline_order : -self.spline_order], noise)\n",
        "            )\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x):\n",
        "        grid = self.grid\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = ((x - grid[:, :-(k+1)]) / (grid[:, k:-1] - grid[:, :-(k+1)]) * bases[:, :, :-1]\n",
        "                     + (grid[:, k+1:] - x) / (grid[:, k+1:] - grid[:, 1:-k]) * bases[:, :, 1:])\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x, y):\n",
        "        A = self.b_splines(x).transpose(0, 1)\n",
        "        B = y.transpose(0, 1)\n",
        "        solution = torch.linalg.lstsq(A, B).solution\n",
        "        return solution.permute(2, 0, 1).contiguous()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(\n",
        "            self.b_splines(x).view(x.size(0), -1),\n",
        "            self.spline_weight.view(self.out_features, -1)\n",
        "        )\n",
        "        return base_output + spline_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FKAN_ResNet(nn.Module):\n",
        "  def __init__(self, num_classes=2, softmax=True):\n",
        "    super(FKAN_ResNet, self).__init__()\n",
        "    self.backbone = torchvision.models.resnet50(pretrained=True)\n",
        "    self.kan_layer1 = KANLinear_v1(1000, 256)\n",
        "    self.kan_layer2 = KANLinear_v1(256, 128)\n",
        "    self.kan_layer3 = KANLinear_v1(128, num_classes)\n",
        "    self.softmax = torch.nn.Softmax(dim=1) if softmax else None\n",
        "    self.change_conv1()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.backbone(x)\n",
        "    x = self.kan_layer1(x)\n",
        "    x = self.kan_layer2(x)\n",
        "    x = self.kan_layer3(x)\n",
        "    if self.softmax:\n",
        "      x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "  def change_conv1(self):\n",
        "      original_conv1 = self.backbone.conv1\n",
        "\n",
        "      #Create a new convolutional layer with 1 input channel instead of 3\n",
        "      new_conv1 = nn.Conv2d(\n",
        "        in_channels=1,  # Grayscale has 1 channel\n",
        "        out_channels=original_conv1.out_channels,\n",
        "        kernel_size=original_conv1.kernel_size,\n",
        "        stride=original_conv1.stride,\n",
        "        padding=original_conv1.padding,\n",
        "        bias=original_conv1.bias is not None\n",
        ")\n",
        "\n",
        "      # Initialize the new conv layer's weights by averaging the RGB weights\n",
        "      with torch.no_grad():\n",
        "        new_conv1.weight = nn.Parameter(original_conv1.weight.mean(dim=1, keepdim=True))\n",
        "\n",
        "        #Replace the original conv1 with the new one\n",
        "        self.backbone.conv1 = new_conv1\n",
        "\n",
        "\n",
        "model = FKAN_ResNet(num_classes=2, softmax=True)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDehAi3WS1xR",
        "outputId": "35953edc-08ba-4e88-efc7-5c22ff910e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6214, 1, 224, 224]) torch.Size([1046, 1, 224, 224])\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert the images and labels to PyTorch tensors\n",
        "\n",
        "# Apply the transformation to training and validation images\n",
        "train_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in train_images]).unsqueeze(1)\n",
        "val_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in val_images]).unsqueeze(1)\n",
        "\n",
        "# Now permute them\n",
        "train_images_tensor = train_images_tensor.permute(0, 1, 2, 3)  # (N, 1, 244, 244)\n",
        "val_images_tensor = val_images_tensor.permute(0, 1, 2, 3)      # (N, 1, 244, 244)\n",
        "print(train_images_tensor.shape, val_images_tensor.shape)\n",
        "\n",
        "# The tensors are now in the shape (N, 1, 244, 244), where N is the number of images\n",
        "\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "# Create the dataset and DataLoader\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "\n",
        "# Define the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "print('Done!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wCqaba_Xu59"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "oGlRJ8QtTziF",
        "outputId": "cfedb528-a58b-42d7-869f-d76c57998539"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2 - Loss: 0.3706498853478235: 100%|██████████| 388/388 [24:14<00:00,  3.75s/it] \n",
            "Epoch 2/2 - Loss: 0.35179195046117623: 100%|██████████| 388/388 [24:47<00:00,  3.83s/it]\n",
            "Epoch 2/2 - Validation Batch: 130: 100%|██████████| 131/131 [01:13<00:00,  1.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/2 - Validation Accuracy: 0.9780114722753346 - Validation F1 Score: 0.9780\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory /content/drive/MyDrive/model_results does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[1;32m     49\u001b[0m     best_validation_score \u001b[38;5;241m=\u001b[39m validation_f1_score\n\u001b[1;32m     50\u001b[0m     patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/model_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_resnet.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     patience_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torch/serialization.py:627\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    624\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torch/serialization.py:501\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/FLEnv/lib/python3.10/site-packages/torch/serialization.py:472\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/MyDrive/model_results does not exist."
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class or binary classification\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)  # AdamW with L2 regularization\n",
        "\n",
        "# Now the data is ready for training and validation\n",
        "\n",
        "# Function to calculate relevant metrics\n",
        "\n",
        "# Training function with Early Stopping\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
        "    patience_counter = 0\n",
        "    best_validation_score = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        p_bar = tqdm(train_loader)\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(p_bar):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            p_bar.set_description(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss / (i + 1)}\")\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            model.eval()\n",
        "            p_bar = tqdm(val_loader)\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "            with torch.no_grad():\n",
        "                for i, (images, labels) in enumerate(p_bar):\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    p_bar.set_description(f'Epoch {epoch+1}/{num_epochs} - Validation Batch: {i}')\n",
        "\n",
        "            class_report = classification_report(all_labels, all_preds, target_names=['Pneumonia', 'Normal'], output_dict=True)\n",
        "            validation_accuracy = class_report['accuracy']\n",
        "            validation_f1_score = class_report['weighted avg']['f1-score']\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Validation Accuracy: {validation_accuracy} - Validation F1 Score: {validation_f1_score:.4f}\")\n",
        "            if validation_f1_score > best_validation_score:\n",
        "                best_validation_score = validation_f1_score\n",
        "                patience_counter = 0\n",
        "                torch.save(model.state_dict(), os.path.join('best_model_resnet.pth'))\n",
        "\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "# Start training\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=2, patience=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ-Sw9n_Xoxx"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW6M8ESPYt7f",
        "outputId": "cf1e4cfa-91c9-457e-abe2-f2e89063515e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-5e523879719c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/drive/MyDrive/model_results/best_model_resnet.pth')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_dict = torch.load('/content/drive/MyDrive/model_results/best_model_resnet.pth')\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAvunQT-Xtwx",
        "outputId": "9bd8527a-3926-4bca-ed07-e7ac58fc0dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([624, 1, 224, 224])\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Pneumonia       0.77      1.00      0.87       390\n",
            "      Normal       0.99      0.51      0.67       234\n",
            "\n",
            "    accuracy                           0.81       624\n",
            "   macro avg       0.88      0.75      0.77       624\n",
            "weighted avg       0.85      0.81      0.80       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in test_images]).unsqueeze(1)  # Applying the same transformation as for train/val\n",
        "test_images_tensor = test_images_tensor.permute(0, 1, 2, 3)\n",
        "print(test_images_tensor.shape)\n",
        "\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)  # or torch.float if binary classification\n",
        "\n",
        "# Create the dataset and DataLoader for the test set\n",
        "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for images, labels in test_loader:\n",
        "  images, labels = images.to(device), labels\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  all_predictions.extend(preds.cpu().numpy())\n",
        "  all_labels.extend(labels.numpy())\n",
        "\n",
        "class_report = classification_report(all_labels, all_predictions, target_names=['Pneumonia', 'Normal'])\n",
        "print(class_report)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FLEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
