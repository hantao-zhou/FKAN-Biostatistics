{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT4iG--NOpTw",
        "outputId": "60cd962b-5123-426f-be02-7e87cc0d6931"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "#from google.colab import drive\n",
        "import albumentations as A\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11680mFSNiEV",
        "outputId": "de630698-5cd8-41f6-9bc7-012326fc2045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3875/3875 [00:07<00:00, 502.77it/s]\n",
            "100%|██████████| 1341/1341 [00:10<00:00, 131.73it/s]\n",
            "100%|██████████| 390/390 [00:00<00:00, 585.38it/s]\n",
            "100%|██████████| 234/234 [00:00<00:00, 246.04it/s]\n",
            "100%|██████████| 8/8 [00:00<00:00, 587.38it/s]\n",
            "100%|██████████| 8/8 [00:00<00:00, 230.07it/s]\n"
          ]
        }
      ],
      "source": [
        "labels = ['PNEUMONIA', 'NORMAL']\n",
        "img_size = 224\n",
        "\n",
        "def get_training_data(data_dir):\n",
        "    data = []\n",
        "\n",
        "    for label in labels:\n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                # Load and resize the image\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size))  # Resize the image\n",
        "\n",
        "                # Add the image and label as a pair\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img}: {e}\")\n",
        "\n",
        "    # Convert the list to a NumPy array\n",
        "    data = np.array(data, dtype=object)  # Use dtype=object to allow image-label pairing\n",
        "    return data\n",
        "\n",
        "# Load the data\n",
        "train_data = get_training_data('/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/chest_xray/train')\n",
        "test_data = get_training_data('/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/chest_xray/test')\n",
        "val_data = get_training_data('/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/chest_xray/val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrf_UaBJOHyK",
        "outputId": "d800b490-6854-4753-ae82-b6625eed088b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of normalized and shuffled train images: (5216, 224, 224)\n",
            "Shape of normalized and shuffled validation images: (16, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Function to normalize the images\n",
        "def normalize_images(data):\n",
        "    images = []\n",
        "    labels = []\n",
        "    \n",
        "    for img, label in data:\n",
        "        # Normalization: each pixel is divided by 255\n",
        "        normalized_img = img / 255.0\n",
        "        images.append(normalized_img)\n",
        "        labels.append(label)\n",
        "    \n",
        "    # Convert the images and labels into separate arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "    return images, labels\n",
        "\n",
        "# Normalize the images in the training dataset\n",
        "train_images, train_labels = normalize_images(train_data)\n",
        "val_images, val_labels = normalize_images(val_data)\n",
        "test_images, test_labels = normalize_images(test_data)\n",
        "\n",
        "# Shuffle the training and validation data\n",
        "train_images, train_labels = shuffle(train_images, train_labels, random_state=42)\n",
        "val_images, val_labels = shuffle(val_images, val_labels, random_state=42)\n",
        "\n",
        "# Check the shape and an example of the normalized and shuffled data\n",
        "print(f\"Shape of normalized and shuffled train images: {train_images.shape}\")\n",
        "print(f\"Shape of normalized and shuffled validation images: {val_images.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Identify the images of the \"Normal\" and \"Pneumonia\" classes\n",
        "normal_images = train_images[train_labels == 1]  # Only \"Normal\" images\n",
        "normal_labels = train_labels[train_labels == 1]  # Corresponding labels\n",
        "pneumonia_images = train_images[train_labels == 0]  # Only \"Pneumonia\" images\n",
        "pneumonia_labels = train_labels[train_labels == 0]  # Corresponding labels\n",
        "\n",
        "# Add a dimension for the channel (1 for grayscale images)\n",
        "normal_images = np.expand_dims(normal_images, axis=-1)\n",
        "pneumonia_images = np.expand_dims(pneumonia_images, axis=-1)\n",
        "\n",
        "# Determine the number of target images to balance the dataset\n",
        "target_normal_images_count = pneumonia_images.shape[0]\n",
        "current_normal_images_count = normal_images.shape[0]\n",
        "images_to_generate = target_normal_images_count - current_normal_images_count\n",
        "\n",
        "# Create an ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Generate augmented images to balance the dataset\n",
        "augmented_normal_images = []\n",
        "if images_to_generate > 0:\n",
        "    augmented_data_gen = datagen.flow(normal_images, normal_labels, batch_size=32, shuffle=False)\n",
        "    for _ in range(images_to_generate // 32 + 1):\n",
        "        batch_images, _ = next(augmented_data_gen)\n",
        "        augmented_normal_images.append(batch_images)\n",
        "\n",
        "    # Concatenate all the generated images\n",
        "    augmented_normal_images = np.vstack(augmented_normal_images)[:images_to_generate]\n",
        "else:\n",
        "    augmented_normal_images = np.empty((0, *normal_images.shape[1:]))\n",
        "\n",
        "# Concatenate the original and augmented images for the \"Normal\" class\n",
        "balanced_normal_images = np.concatenate([normal_images, augmented_normal_images])\n",
        "balanced_normal_labels = np.full(balanced_normal_images.shape[0], 1)\n",
        "\n",
        "# Create the final balanced dataset\n",
        "augmented_train_images = np.concatenate([pneumonia_images, balanced_normal_images])\n",
        "augmented_train_labels = np.concatenate([pneumonia_labels, balanced_normal_labels])\n",
        "\n",
        "# Shuffle the dataset randomly\n",
        "augmented_train_images, augmented_train_labels = shuffle(augmented_train_images, augmented_train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhWclu08OZ6l",
        "outputId": "ae97f186-7e21-470c-c5bc-f5521ae1829d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2, softmax=True):\n",
        "      super(ResNet, self).__init__()\n",
        "      self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "      num_ftrs = self.resnet.fc.out_features\n",
        "      self.fc = nn.Linear(num_ftrs, num_classes)\n",
        "      self.bn = nn.BatchNorm1d(num_ftrs)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.softmax = torch.nn.Softmax(dim=1) if softmax else None\n",
        "      self.change_conv1()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.resnet(x)\n",
        "      x = self.bn(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc(x)\n",
        "      if self.softmax:\n",
        "        x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "    def change_conv1(self):\n",
        "      original_conv1 = self.resnet.conv1\n",
        "\n",
        "      #Create a new convolutional layer with 1 input channel instead of 3\n",
        "      new_conv1 = nn.Conv2d(\n",
        "        in_channels=1,  # Grayscale has 1 channel\n",
        "        out_channels=original_conv1.out_channels,\n",
        "        kernel_size=original_conv1.kernel_size,\n",
        "        stride=original_conv1.stride,\n",
        "        padding=original_conv1.padding,\n",
        "        bias=original_conv1.bias is not None\n",
        ")\n",
        "\n",
        "      # Initialize the new conv layer's weights by averaging the RGB weights\n",
        "      with torch.no_grad():\n",
        "        new_conv1.weight = nn.Parameter(original_conv1.weight.mean(dim=1, keepdim=True))\n",
        "\n",
        "        #Replace the original conv1 with the new one\n",
        "        self.resnet.conv1 = new_conv1\n",
        "\n",
        "class KANLinear_v2(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=10, spline_order=3,\n",
        "                 scale_noise=0.05, scale_base=1.0, scale_spline=1.0,\n",
        "                 enable_standalone_scale_spline=True, base_activation=nn.ReLU,\n",
        "                 grid_eps=0.01, grid_range=[0, 1], dropout_prob=0.2):\n",
        "        super(KANLinear_v2, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = ((torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                 + grid_range[0]).expand(in_features, -1).contiguous())\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(\n",
        "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
        "        )\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = ((torch.rand(self.grid_size + 1, self.in_features, self.out_features) - 0.5)\n",
        "                     * self.scale_noise / self.grid_size)\n",
        "            self.spline_weight.data.copy_(\n",
        "                self.scale_spline * self.curve2coeff(self.grid.T[self.spline_order : -self.spline_order], noise)\n",
        "            )\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x):\n",
        "        grid = self.grid\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = ((x - grid[:, :-(k+1)]) / (grid[:, k:-1] - grid[:, :-(k+1)]) * bases[:, :, :-1]\n",
        "                     + (grid[:, k+1:] - x) / (grid[:, k+1:] - grid[:, 1:-k]) * bases[:, :, 1:])\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x, y):\n",
        "        A = self.b_splines(x).transpose(0, 1)\n",
        "        B = y.transpose(0, 1)\n",
        "        solution = torch.linalg.lstsq(A, B).solution\n",
        "        return solution.permute(2, 0, 1).contiguous()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(\n",
        "            self.b_splines(x).view(x.size(0), -1),\n",
        "            self.spline_weight.view(self.out_features, -1)\n",
        "        )\n",
        "        spline_output = self.dropout(spline_output)  # Dropout per generalizzazione\n",
        "        return base_output + spline_output\n",
        "\n",
        "class FKAN_ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2, softmax=True):\n",
        "        super(FKAN_ResNet, self).__init__()\n",
        "        self.backbone = torchvision.models.resnet18(pretrained=True)\n",
        "        self.kan_layer1 = KANLinear_v2(256, 128, grid_size=8)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.kan_layer2 = KANLinear_v2(128, 64, grid_size=8)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.kan_layer3 = KANLinear_v2(64, num_classes, grid_size=5)\n",
        "        self.softmax = torch.nn.Softmax(dim=1) if softmax else None\n",
        "        self.change_conv1()\n",
        "        self.modify_fc_layer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.kan_layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.kan_layer2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.kan_layer3(x)\n",
        "        if self.softmax:\n",
        "            x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "    def change_conv1(self):\n",
        "        original_conv1 = self.backbone.conv1\n",
        "        new_conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=original_conv1.out_channels,\n",
        "            kernel_size=original_conv1.kernel_size,\n",
        "            stride=original_conv1.stride,\n",
        "            padding=original_conv1.padding,\n",
        "            bias=original_conv1.bias is not None\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new_conv1.weight = nn.Parameter(original_conv1.weight.mean(dim=1, keepdim=True))\n",
        "        self.backbone.conv1 = new_conv1\n",
        "\n",
        "    def modify_fc_layer(self):\n",
        "        num_ftrs = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(num_ftrs, 256)\n",
        "\n",
        "model = FKAN_ResNet(num_classes=2, softmax=True)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           3,136\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                  [-1, 256]         131,328\n",
            "           ResNet-69                  [-1, 256]               0\n",
            "             ReLU-70                  [-1, 256]               0\n",
            "          Dropout-71                  [-1, 128]               0\n",
            "     KANLinear_v2-72                  [-1, 128]               0\n",
            "      BatchNorm1d-73                  [-1, 128]             256\n",
            "             ReLU-74                  [-1, 128]               0\n",
            "          Dropout-75                   [-1, 64]               0\n",
            "     KANLinear_v2-76                   [-1, 64]               0\n",
            "      BatchNorm1d-77                   [-1, 64]             128\n",
            "             ReLU-78                   [-1, 64]               0\n",
            "          Dropout-79                    [-1, 2]               0\n",
            "     KANLinear_v2-80                    [-1, 2]               0\n",
            "          Softmax-81                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 11,301,952\n",
            "Trainable params: 11,301,952\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 62.80\n",
            "Params size (MB): 43.11\n",
            "Estimated Total Size (MB): 106.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (1, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDehAi3WS1xR",
        "outputId": "35953edc-08ba-4e88-efc7-5c22ff910e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([7750, 1, 224, 224]) torch.Size([16, 1, 224, 224])\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert the images and labels to PyTorch tensors\n",
        "\n",
        "# Apply the transformation to training and validation images\n",
        "train_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in augmented_train_images])\n",
        "val_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in val_images]).unsqueeze(1)\n",
        "\n",
        "# Now permute them\n",
        "train_images_tensor = train_images_tensor.permute(0, 3, 1, 2)  # (N, 1, 244, 244)\n",
        "val_images_tensor = val_images_tensor.permute(0, 1, 2, 3)      # (N, 1, 244, 244)\n",
        "print(train_images_tensor.shape, val_images_tensor.shape)\n",
        "\n",
        "# The tensors are now in the shape (N, 1, 244, 244), where N is the number of images\n",
        "\n",
        "train_labels_tensor = torch.tensor(augmented_train_labels, dtype=torch.long)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "# Create the dataset and DataLoader\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "\n",
        "# Define the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wCqaba_Xu59"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "oGlRJ8QtTziF",
        "outputId": "cfedb528-a58b-42d7-869f-d76c57998539"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - Loss: 0.34874547703945935: 100%|██████████| 484/484 [29:48<00:00,  3.70s/it]\n",
            "Epoch 2/100 - Loss: 0.3417824539394418: 100%|██████████| 484/484 [29:09<00:00,  3.62s/it] \n",
            "Epoch 2/100 - Validation Batch: 1: 100%|██████████| 2/2 [00:00<00:00,  3.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 - Validation Accuracy: 0.6875 - Validation F1 Score: 0.6537\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100 - Loss: 0.3526126408860797:  52%|█████▏    | 252/484 [12:19:27<11:20:46, 176.06s/it] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb Cella 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
            "\u001b[1;32m/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb Cella 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "\u001b[1;32m/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb Cella 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkan_layer1(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/giuliasaresini/Documents/tesi_vs/Medicine/Progetto/fkan_resnet_model.ipynb#X11sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkan_layer2(x)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torchvision/models/resnet.py:154\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m--> 154\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(out)\n\u001b[1;32m    155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/Documents/tesi_vs/bio_vero/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    551\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class or binary classification\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)  # AdamW with L2 regularization\n",
        "\n",
        "# Now the data is ready for training and validation\n",
        "\n",
        "# Function to calculate relevant metrics\n",
        "\n",
        "# Training function with Early Stopping\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
        "    patience_counter = 0\n",
        "    best_validation_score = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        p_bar = tqdm(train_loader)\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(p_bar):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            p_bar.set_description(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss / (i + 1)}\")\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            model.eval()\n",
        "            p_bar = tqdm(val_loader)\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "            with torch.no_grad():\n",
        "                for i, (images, labels) in enumerate(p_bar):\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    p_bar.set_description(f'Epoch {epoch+1}/{num_epochs} - Validation Batch: {i}')\n",
        "\n",
        "            class_report = classification_report(all_labels, all_preds, target_names=['Pneumonia', 'Normal'], output_dict=True)\n",
        "            validation_accuracy = class_report['accuracy']\n",
        "            validation_f1_score = class_report['weighted avg']['f1-score']\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Validation Accuracy: {validation_accuracy} - Validation F1 Score: {validation_f1_score:.4f}\")\n",
        "            if validation_f1_score > best_validation_score:\n",
        "                best_validation_score = validation_f1_score\n",
        "                patience_counter = 0\n",
        "                torch.save(model.state_dict(), os.path.join('best_model_resnet.pth'))\n",
        "\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "# Start training\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ-Sw9n_Xoxx"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW6M8ESPYt7f",
        "outputId": "cf1e4cfa-91c9-457e-abe2-f2e89063515e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-5e523879719c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/drive/MyDrive/model_results/best_model_resnet.pth')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_dict = torch.load('/content/drive/MyDrive/model_results/best_model_resnet.pth')\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAvunQT-Xtwx",
        "outputId": "9bd8527a-3926-4bca-ed07-e7ac58fc0dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([624, 1, 224, 224])\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Pneumonia       0.77      1.00      0.87       390\n",
            "      Normal       0.99      0.51      0.67       234\n",
            "\n",
            "    accuracy                           0.81       624\n",
            "   macro avg       0.88      0.75      0.77       624\n",
            "weighted avg       0.85      0.81      0.80       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_images_tensor = torch.stack([torch.tensor(img, dtype=torch.float) for img in test_images]).unsqueeze(1)  # Applying the same transformation as for train/val\n",
        "test_images_tensor = test_images_tensor.permute(0, 1, 2, 3)\n",
        "print(test_images_tensor.shape)\n",
        "\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)  # or torch.float if binary classification\n",
        "\n",
        "# Create the dataset and DataLoader for the test set\n",
        "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "for images, labels in test_loader:\n",
        "  images, labels = images.to(device), labels\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  all_predictions.extend(preds.cpu().numpy())\n",
        "  all_labels.extend(labels.numpy())\n",
        "\n",
        "class_report = classification_report(all_labels, all_predictions, target_names=['Pneumonia', 'Normal'])\n",
        "print(class_report)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FLEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
